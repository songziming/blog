---
title: "Wheel 调度算法"
tags: []
---

本文介绍开发 Wheel 使用的任务调度算法。目前 Wheel 仅仅支持内核态任务的调度，所有的任务共享内核地址空间，但调度的策略对进程和线程都是一致的。

主要从参考的调度器有三个：

- VxWorks 的实时调度器
- Linux CFS 调度算法
- FreeBSD ULE 调度算法

Wheel 对任务的调度是按照优先级进行的，而且支持 SMP 硬件。也就是说，所有就绪态任务按优先级从高到低排序，前 N 个任务可以同时运行。随着功能的完善，还要加入 CPU 亲和性支持。

### 什么时候切换任务

调度的目的是切换任务，任务的切换有两种原因，要么是因为就绪队列发生改动而触发主动任务切换，要么是因为当前任务时间片用完需要进行时间片轮转。如果是时间片用完，那么比较容易，只要把当前任务放到队尾，从队头取出一个新元素就可以了。如果是就绪队列改变，那么就需要首先提取最高的优先级，然后判断能够抢占当前的任务。

### 多核的锁问题

多核处理器都可以接收时钟中断，而且是相互独立的。因此在检查时间片轮转时，需要注意同步问题。另外，创建任务、或者改变任务状态时，也有可能引发调度，因此这些函数也要加锁。

有一个比较偷懒的办法，就是让所有数据结构都是 percpu 的。就绪队列每个 CPU 一份，这样在需要调度的时候，就可以直接访问自己这个 CPU 的数据结构，不需要上锁。但完全 percpu 也是不现实的，不然就成了若干个独立的计算机，而不是SMP。既然是 SMP，特点就是任务可以在各个 CPU 之间迁移。如果不迁移，各个CPU上运行的任务可能会不平衡，导致系统不公平。

vxworks的做法是创建一个全局的就绪队列，保存那些非亲和任务，并且还有N个亲和队列。每个CPU检查自己的亲和队列以及全局队列，找出优先级最高的任务，然后执行。但是这样会竞争全局队列。Linux和BSD没有全局队列，但是定期执行负载均衡操作。

负载均衡的做法只适用于非实时任务，因为没有了全局队列，高优先级的任务可能被分配给不正确的CPU。优先级规定，有N个CPU，那么当前优先级最高的N和任务就要执行。如果保证优先级最高的N个能执行？如果这N个任务恰好放在了同一个CPU的就绪队列中，那么其他CPU是无法访问这N个任务的。

我们可以用一个变量，来记录每个CPU当前最高优先级（通常也就是tid_current的优先级），如果创建了一个新任务，就找出当前优先级最低的那个CPU。每次一个CPU的当前优先级发生改变，我们就要更新所有CPU优先级关系。由于我们只关心哪个CPU的优先级最高，可以用一个最小堆来实现。如果一个CPU的优先级提升了（即取值减小），那么就会沿着二叉堆上升。如果一个CPU上当前任务阻塞，开始运行一个更低优先级的任务，那么这个CPU就会沿着树下降。
记录哪个CPU优先级最高，有什么作用？作用就是，当我们创建一个新的高优先级任务的时候，决定抢占哪个处理器的。

负载均衡一定要定期执行吗，由于负载不均衡的情况只在操作就绪队列的时候出现，因此如果一直没有操作就绪队列，负载就不会不均衡。所以，只有连续操作了几次就绪队列，才需要进行负载均衡。

运行状态的任务是不在就绪队列里面的，

### 多核上的相关情况

#### 代价

有两个操作对性能有严重影响：任务迁移会导致缓存失效，让任务运行速度下降；不迁移任务，有可能导致某些CPU空闲。

#### 创建新任务

每创建一个新的任务时，我们都要为这个任务挑选一个CPU。为了让CPU的负载尽量均衡，显然应该选择当前任务数最少的那个CPU。

#### 如何进行负载均衡

有两种办法：pull/push。pull方法就是，一个idle状态的CPU从其他CPU中“偷取”任务；push方法就是，有一个周期性的任务，不断检查负载情况，然后让各个CPU尽量平衡。这两种方法是可以共存的。

pull方法存在一个问题，那就是它只会在CPU完全空闲的情况下尝试负载均衡。如果CPU一直不空闲，只是任务数量很少，那按理也应该执行负载均衡。因此需要一个主动的线程，在ULE中是每2s执行一次，负责挑出负载差异最大的两个CPU，在它们之间迁移任务。因为我们要让各个CPU的差异最小，也就是极差最小，所有仅仅操作最极端的两个，显然是最靠谱的。

> 是否有必要保证任务之间的公平性，程序员在开发的时候应该考虑到这个问题，不能假定多个任务运行速度相同。
>
> 如果任务数量没法平均分到各个CPU上，那么就会存在一定程度的不公平，但我们认为这是可以接受的。

### 无锁算法？

不知道能不能实现一套无锁的算法，用atomic操作，然后只用一个就绪队列。
我们操作就绪队列，基本上就两个操作——取最前面的任务、将新的任务放进区。能不能让这两个操作一步完成？

// 假设就绪队列是单链表，但是queue记录了head和tail
// 向表尾追加node节点
tail = atomic_get(queue.tail);
tail->next = node;
node->next = NULL;
atomic_cas(queue.tail, tail, node);
// 如果最后一步的cas没有成功，就重新之上上面的操作。

只要我们不修改 queue.tail 的取值，这个队列的结构就没有变。我们在定义就绪队列时，指定了首尾元素，头节点的prev指针与尾节点的next指针均无意义。

// 向表头添加一个新的元素，如果是创建一个新任务，那么就应该添加到链表头，这样才能在最短时间内开始运行
head = atomic_get(queue.head);
node->next = head;
atomic_cas(queue.head, head, node); // 若失败则循环

调度的时候，还需要取出一个元素：

head = atomic_get(queue.head)
node = head->next;
atomic_cas(queue.head, head, node);

当然，调度时候不是单纯的取出第一个元素即可，还要检查优先级、检查禁用抢占。因此，获取了head之后，可能首先会做很多检查工作，然后再将这个节点从链表里取出。

但是需要明白的是，一个CPU上执行这样的工作，另一个CPU上也一样。有可能两个CPU取到了相同的 atomic_get结果。所以，我们在判断过程中，只能读head的信息，但绝对不能修改。

这个方法虽然无锁，但也有循环的存在，相当于一个变相的自旋锁。而且，这个自旋锁争用的queue.head同时处于多个CPU的缓存中，一个CPU执行更新，其他CPU的缓存都要失效，并没有queued-spinlock这样的好处。如果是这样，那这个无锁算法还是否有意义？

实际的自旋锁还有优先级掩码，用一个个比特表示哪些优先级存在。如何快速判断哪个CPU的优先级最高（数值最小）：

mask = cpu[i].priority_mask
x = mask ^ (mask - 1)   // 最低位，以及后面的所有bit都为1，其他bit均为0

把所有CPU的x共同作与运算，最终的结果就是最小的优先级。然后分别和mask执行与运算，如果结果中恰好有一个bit为1，说明
这个CPU具有系统中最高的优先级。

也就是说，我们需要一个map操作，同时对所有CPU的变量执行操作。这个操作其实难以实现，因为我们要保证每个CPU的mask都保持不变。整个过程，如果有一个CPU的mask改变了，那将会影响

### 一次调度，给出所有CPU的结果

如果我们使用一个单独的就绪队列，每当发生变更，就要执行调度。但是，我们在修改就绪队列的同时，就为所有CPU算出了它们应该运行的任务。

为了不影响当前正在运行的任务，我们显然不能用变量 tid_current，不妨借鉴双缓冲的思路，再增加一个变量 tid_next。每次中断返回，要做的就是将 tid_current 置为 tid_next 的值，而且是无条件的。因此，设置 tid_next也就相当于`sched_later`。

进入中断时，将寄存器保存在 tid_current。从中断返回时，从 tid_next 处读出寄存器的值，并且将 tid_next 的值写入 tid_current。这种设计使得中断过程变得及其简单。

在一个4核的计算机上，CPU0 上面任务状态发生改变，操作了全局的就绪队列，因此要计算出 cpu0~3 接下来要运行的任务，并修改它们的 tid_next。由于各个CPU是并行的，cpu0在计算核更新的时候，不知道其他cpu处于什么状态，是否在中断内。

而且，就绪队列的一次变更，应该最多影响一个CPU，就是优先级最高/最低的那个CPU。
- 创建一个新任务，这个新任务应该交给优先级最低的那个 CPU 运行（前提是如果能抢占的话，如果连这个最低的优先级都不够，那么新创建的任务就只能放在就绪队列里面等着）
- 某个CPU上正在运行的任务退出，或者进入阻塞状态，那么这个 CPU 应该从就绪队列中挑出队首的任务运行。

使用一个就绪队列，在时间片轮转的时候会有问题，任务有可能在各个CPU之间来回切换。

