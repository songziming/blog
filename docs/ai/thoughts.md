关于神经网络的一些想法。

### 非线性拟合

很多文献都说神经网络具有很强的非线性拟合功能，这到底是什么意思。

神经网络的计算过程转换成公式，应该是个这样子：

$$ \vec{y} = f( W_{N} \cdot f( W_{N-1} \cdot f( \cdots f( W_1 \cdot \vec{x} ) ) ) ) $$

其中$W$是各层之间的权值矩阵，$f$是激活函数。

如果激活函数全都换成$y=x$，那么神经网络就成了若干个矩阵连乘，由于矩阵乘法遵循结合律，最终网络就变成了

$$ \vec{y}=W\vec{x} $$

这种矩阵方程，就等价于N元一次方程组，是线性的，因此如果想让神经网络拟合非线性函数，关键就在于激活函数，只有激活函数的加入，才能将神经网络真正变为一个非线性拟合器。

### 归一化

训练数据要进行归一化，我在测试BP网络的时候已经体会到这点了。但是归一化也有不同方法，可以使用MinMaxScaler，将所有数据都放在$[0，1]$的区间之内，也可以让最终数据均值为0，方差为1。

个人感觉，权值应该允许取值为负，不然，前一层神经元的数量就会对本层神经元输入产生较大影响。如果一切都是正的，那么在加权求和的时候，当前神经元的输入值就会偏大。

如果输入数据的均值是0，而且激活函数取值也是[-1,1]区间的，那么就可以保证网络计算过程中没有累计偏移。均值为0的数据经过激活函数的处理，均值仍然是0。

- - -

### Gradient Checking

如何验证神经网络的实现是否正确？当我写出BP代码之后，最直接的想法就是找一些数据集测试一下。但是发现某些参数情况下，误差随着重复训练反而升高了。

查阅资料发现，梯度检查是专门用来检验BP实现的方法，正是因为误差反传比较容易出错，所以人们专门总结出了这个检测方法。有的时候，代码中可能存在一些问题，但是在特定数据集之下，这些问题不能显现出来。

[Debugging: Gradient Checking
](http://ufldl.stanford.edu/tutorial/supervised/DebuggingGradientChecking/)

BP网络的训练过程就是最小化误差，更通用一点，最小化代价函数。